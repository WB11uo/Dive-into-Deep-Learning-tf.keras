{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.8. 网络中的网络(NiN).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chongzicbo/Dive-into-Deep-Learning-tf.keras/blob/master/5.8.%20%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C(NiN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdL_2Jqeen-H",
        "colab_type": "text"
      },
      "source": [
        "##5.8. 网络中的网络（NiN）\n",
        "前几节介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。本节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n",
        "\n",
        "###5.8.1. NiN块\n",
        "我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。回忆在“多输入通道和多输出通道”一节里介绍的$1\\times1$卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用$1\\times1$卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。图5.7对比了NiN同AlexNet和VGG等网络在结构上的主要区别。\n",
        "\n",
        "<center><img src="https://zh.gluon.ai/_images/nin.svg" width="300"/></center>\n",
        "<center>图 5.7 左图是AlexNet和VGG的网络结构局部，右图是NiN的网络结构局部</center>\n",
        "\n",
        "NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的1×1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4lu5nU49D5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,Sequential\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras import losses,optimizers\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorflow import image\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-YL4jhLXztp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution() #启用动态图计算，tf2.x 不需要调用此函数"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlhUo8FtUD98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nin_block(num_channels,kernel_size,strides,padding):\n",
        "  layer_ls=[\n",
        "    layers.Convolution2D(num_channels,kernel_size=kernel_size,strides=strides,padding='same',activation='relu'),\n",
        "    layers.Convolution2D(num_channels,kernel_size=1,activation='relu'),\n",
        "    layers.Convolution2D(num_channels,kernel_size=1,activation='relu')\n",
        "  ]\n",
        "  blk=Sequential(layer_ls)\n",
        "  return blk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDytjevbfSVj",
        "colab_type": "text"
      },
      "source": [
        "###5.8.2. NiN模型\n",
        "NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为$11\\times11$、$5\\times5$和$3\\times3$的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为$3\\times3$的最大池化层。\n",
        "\n",
        "除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHMeup3nUwqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net=Sequential()\n",
        "net.add(nin_block(96,kernel_size=11,strides=4,padding='same'))\n",
        "net.add(layers.MaxPool2D(pool_size=3,strides=2))\n",
        "net.add(nin_block(256,kernel_size=5,strides=1,padding='same'))\n",
        "net.add(layers.MaxPool2D(pool_size=3,strides=2))\n",
        "net.add(nin_block(384,kernel_size=3,strides=1,padding='same'))\n",
        "net.add(layers.MaxPool2D(pool_size=3,strides=2))\n",
        "net.add(layers.Dropout(0.5))\n",
        "net.add(nin_block(10,kernel_size=3,strides=1,padding='same'))#标签类别数为10\n",
        "net.add(layers.GlobalAveragePooling2D()) #全局平均池化会将数据转换成形状(batch_size,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr3lAwV-fjc5",
        "colab_type": "text"
      },
      "source": [
        "我们构建一个数据样本来查看每一层的输出形状。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFWFTrwmWdZY",
        "colab_type": "code",
        "outputId": "201852de-8d23-4705-941a-b93a953e18cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "X=tf.random.uniform(shape=(1,224,224,1))\n",
        "for layer in net.layers:\n",
        "  X=layer(X)\n",
        "  print(layer.name,'output shape:\\t',X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequential_1 output shape:\t (1, 56, 56, 96)\n",
            "max_pooling2d output shape:\t (1, 27, 27, 96)\n",
            "sequential_2 output shape:\t (1, 27, 27, 256)\n",
            "max_pooling2d_1 output shape:\t (1, 13, 13, 256)\n",
            "sequential_3 output shape:\t (1, 13, 13, 384)\n",
            "max_pooling2d_2 output shape:\t (1, 6, 6, 384)\n",
            "dropout output shape:\t (1, 6, 6, 384)\n",
            "sequential_4 output shape:\t (1, 6, 6, 10)\n",
            "global_average_pooling2d output shape:\t (1, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi0nKrTyfndn",
        "colab_type": "text"
      },
      "source": [
        "###5.8.3. 获取数据和训练模型\n",
        "我们依然使用Fashion-MNIST数据集来训练模型。NiN的训练与AlexNet和VGG的类似，但这里使用的学习率更大。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUZm-aZYWrzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr,num_epochs,batch_size=0.05,5,128\n",
        "optimizer=optimizers.SGD(learning_rate=lr)\n",
        "loss=losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "buffer_size=1000\n",
        "def load_data_fashion_mnist(batch_size,buffer_size):\n",
        "  (x_train,y_train),(x_test,y_test)=keras.datasets.fashion_mnist.load_data()\n",
        "  x_train=x_train[:,:,:,np.newaxis]#将三维张量增加一个channel维\n",
        "  x_test=x_test[:,:,:,np.newaxis]\n",
        "  train_iter=Dataset.from_tensor_slices((x_train,y_train)).map(lambda x,y:(x/255,y)).shuffle(buffer_size).batch(batch_size).map(lambda x,y:(image.resize(images=x,size=(224,224)),y))\n",
        "  test_iter=Dataset.from_tensor_slices((x_test,y_test)).map(lambda x,y:(x/255,y)).batch(batch_size).map(lambda x,y:(image.resize(images=x,size=(224,224)),y))\n",
        "  return train_iter,test_iter\n",
        "train_iter,test_iter=load_data_fashion_mnist(batch_size=batch_size,buffer_size=buffer_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK4GlS5FXMSX",
        "colab_type": "code",
        "outputId": "5f9a3aeb-e956-417d-f08a-ef53906f38b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "net.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n",
        "history=net.fit_generator(train_iter,validation_data=test_iter,epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "118/469 [======>.......................] - ETA: 1:24:12 - loss: 2.3016 - acc: 0.1031"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQdykrEDXPAy",
        "colab_type": "text"
      },
      "source": [
        "###5.8.4. 小结\n",
        "* NiN重复使用由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层网络。\n",
        "* NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。\n",
        "* NiN的以上设计思想影响了后面一系列卷积神经网络的设计。"
      ]
    }
  ]
}
